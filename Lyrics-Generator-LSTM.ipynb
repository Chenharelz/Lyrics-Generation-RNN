{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations, Imports, Drive mount and Device selection"
      ],
      "metadata": {
        "id": "bAzbpXoQQc39"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KmJlkEOxkuMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b9fa56-ede3-44c9-cf5c-6386b5faf855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboard-plugin-customizable-plots\n",
            "  Downloading tensorboard_plugin_customizable_plots-0.1.9-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard-plugin-customizable-plots\n",
            "Successfully installed tensorboard-plugin-customizable-plots-0.1.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from pretty_midi) (1.21.6)\n",
            "Collecting mido>=1.1.16\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from pretty_midi) (1.15.0)\n",
            "Building wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591954 sha256=937f0a1118168bd20fc07eb07a6f9d36f39eb641e8bfe5f52ea7c66bd77a4502\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/5a/e3/30eeb9a99350f3f7e21258fcb132743eef1a4f49b3505e76b6\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: mido, pretty_midi\n",
            "Successfully installed mido-1.2.10 pretty_midi-0.2.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (2.9.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.25.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.38.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (3.19.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.21.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.51.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.15.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboard-plugin-customizable-plots\n",
        "!pip install pretty_midi\n",
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GeCFrrvwTw6t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "import re\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as T\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "import random\n",
        "import heapq\n",
        "from operator import itemgetter\n",
        "from torch.nn.modules import dropout\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TnWge6LoQ9hx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472f4330-15e0-48b8-87ee-c8eddac92388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g_Qp61yXLkyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ccd541-a589-4ddc-84a9-35b8cf0f5434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Reading"
      ],
      "metadata": {
        "id": "7--X-vFxQx2V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_FuZqI7CUqDO"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "\n",
        "path_to_assingment_folder = '/content/drive/MyDrive/DeepLearningAssingment3'\n",
        "midi_folder_name = 'midi_files'\n",
        "test_set_csv_name = 'lyrics_test_set.csv'\n",
        "train_set_csv_name = 'lyrics_train_set.csv'\n",
        "path_to_train = os.path.join(path_to_assingment_folder, train_set_csv_name)\n",
        "path_to_test = os.path.join(path_to_assingment_folder, test_set_csv_name)\n",
        "path_to_midi_dir = os.path.join(path_to_assingment_folder, midi_folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NTcgrLa0Ruaf"
      },
      "outputs": [],
      "source": [
        "# Fixing column names and reading train and test data\n",
        "\n",
        "col_names = ['artist', 'song', 'lyrics']\n",
        "train_set_csv = pd.read_csv(path_to_train, names = col_names+[\"a\",\"b\",\"c\",\"d\"])[col_names]\n",
        "test_set_csv = pd.read_csv(path_to_test, names = col_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CyLrtjUbcF9I"
      },
      "outputs": [],
      "source": [
        "# Helper functions for data reading\n",
        "\n",
        "def clean_midi_name(file_name):\n",
        "  '''\n",
        "  This function get a file name and clean it\n",
        "  '''\n",
        "  cleaned_name = file_name.replace('_-_',' ')\n",
        "  cleaned_name = cleaned_name.replace('_',' ')\n",
        "  cleaned_name = cleaned_name.replace('.mid','')\n",
        "  return cleaned_name.lower()\n",
        "\n",
        "def generate_lyrics_midi_lists(df, path_to_midi_dir):\n",
        "  '''\n",
        "  DESCRIPTION: this function generates pairs of lyrics and midi files.\n",
        "  PARAMETERS:\n",
        "  :param df(pandas DataFrame): the data we're working with.\n",
        "  :param path_to_midi_dir(str): the path for the location of the midi files.\n",
        "  :return: list of tuples, each tuple (lyrics, midifile)\n",
        "  '''\n",
        "  midi_file_names = [f for f in os.listdir(path_to_midi_dir) if os.path.isfile(os.path.join(path_to_midi_dir, f))]\n",
        "  midi_file_names_clean = [clean_midi_name(file_name) for file_name in midi_file_names]\n",
        "\n",
        "  mapping = {}\n",
        "  df['file_name'] = df['artist'] + ' ' + df['song']\n",
        "  lyrics_l = []\n",
        "  midis = []\n",
        "  for file_num in range(len(midi_file_names)):\n",
        "    mapping[midi_file_names_clean[file_num]] = midi_file_names[file_num]\n",
        "  for ind, row in df.iterrows():\n",
        "    file_name = \" \".join(row['file_name'].split())\n",
        "    lyrics = row['lyrics']\n",
        "    try: \n",
        "      midi_file_name = mapping[file_name]\n",
        "    except:\n",
        "      continue\n",
        "    midi_file_path = os.path.join(path_to_midi_dir, midi_file_name)\n",
        "    try:\n",
        "      midi_file = pretty_midi.PrettyMIDI(midi_file_path)\n",
        "      lyrics_l.append(lyrics)\n",
        "      midis.append(midi_file)\n",
        "    except: \n",
        "      continue\n",
        "  return lyrics_l, midis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9QgjL2H1lmkY"
      },
      "outputs": [],
      "source": [
        "# Generating lyrics and matching midi lists from train and test dfs\n",
        "try:\n",
        "  midis_train = pd.read_pickle(os.path.join(path_to_assingment_folder ,\"midis_train\"))\n",
        "  lyrics_train = pd.read_pickle(os.path.join(path_to_assingment_folder ,\"lyrics_train\"))\n",
        "except:\n",
        "  lyrics_train, midis_train = generate_lyrics_midi_lists(train_set_csv, path_to_midi_dir)\n",
        "  pd.to_pickle(midis_train ,filepath_or_buffer=os.path.join(path_to_assingment_folder , \"midis_train\"))\n",
        "  pd.to_pickle(lyrics_train ,filepath_or_buffer=os.path.join(path_to_assingment_folder , \"lyrics_train\"))\n",
        "lyrics_test, midis_test = generate_lyrics_midi_lists(test_set_csv, path_to_midi_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess "
      ],
      "metadata": {
        "id": "oqUeZlR6Rib8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "muHQhXsk9nl4"
      },
      "outputs": [],
      "source": [
        "# Helper functions for preprocessing\n",
        "\n",
        "def song_cleaner(song):\n",
        "  '''\n",
        "  DESCRIPTION: this function get a song lyrics and preproccess it to be in the right format.\n",
        "  PARAMETERS:\n",
        "  :param song(string): the lyrics of the song.\n",
        "  :return: song(string): clean song.\n",
        "  '''\n",
        "  song = song.lower()\n",
        "  song = song.replace(\"#\", \"\").replace(\"_\", \" \").replace(\",\", \"\").replace(\"'\", \"\").replace(\".\", \"\").replace(\"?\", \"\").replace(\":\", \"\").replace(\"!\", \"\").replace(\";\", \"\").replace(\"-\", \" \").replace(\"-\", \" \").replace(\"(\", \"\").replace(\")\", \"\").replace(\"`\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"{\",\"\").replace(\"}\",\"\")\n",
        "  return song\n",
        "\n",
        "\n",
        "def get_word2vec():\n",
        "  '''\n",
        "  DESCRIPTION: this function importing pre trained word2vec\n",
        "  :return: word2vec.\n",
        "  '''\n",
        "\n",
        "  try:\n",
        "    word2vec = pd.read_pickle(os.path.join(path_to_assingment_folder ,\"word2vec-google-news-300\"))\n",
        "  except:\n",
        "    word2vec = gensim.downloader.load('word2vec-google-news-300')\n",
        "    pd.to_pickle(word2vec ,filepath_or_buffer=os.path.join(path_to_assingment_folder , \"word2vec-google-news-300\"))\n",
        "  return word2vec\n",
        "  \n",
        "def get_clean_lyrics_list_of_list(lyrics):\n",
        "  '''\n",
        "  DESCRIPTION: this function tokenize the lyrics of each song\n",
        "  PARAMETERS:\n",
        "  :param lyrics([[str],[str],[str]]) : each inner list is a song lyrics string\n",
        "  :return: song(string): tokenized list of lists\n",
        "  '''\n",
        "\n",
        "  clean_lyrics_list = []\n",
        "  for song in lyrics:\n",
        "      clean_lyrics_list.append(song_cleaner(song))\n",
        "  clean_lyrics_list_of_list = [song.split() for song in clean_lyrics_list]\n",
        "  return clean_lyrics_list_of_list\n",
        "\n",
        "\n",
        "def get_embedding_dict_and_embedded_lyrics(clean_lyrics_list_of_lists):\n",
        "  '''\n",
        "  DESCRIPTION: this function is the fundemantal of our embedding layer it take the list of lists\n",
        "                and creating an embedding dict embedded songs and words that was not found in word2vec\n",
        "                but do exist in the lyrics dataset.\n",
        "  PARAMETERS:\n",
        "  :param clean_lyrics_list_of_lists: [[lyrics],[lyrics],[lyrics],[lyrics]]\n",
        "  :return: embedded_dict (key=word,value=embedded vector), embedded_songs [[embedded vector for each word a song],[embedded vector for each word a song]]\n",
        "            words_not_known_to_word2vec which is self explanetory.\n",
        "  '''\n",
        "\n",
        "  try:\n",
        "    embedding_dict = pd.read_pickle(os.path.join(path_to_assingment_folder ,\"embedding_dict\"))\n",
        "    embedded_songs = pd.read_pickle(os.path.join(path_to_assingment_folder ,\"embedded_lyrics\"))\n",
        "    words_not_known_to_word2vec = pd.read_pickle(os.path.join(path_to_assingment_folder ,\"words_not_known_to_word2vec\"))\n",
        "  except:\n",
        "    word2vec = get_word2vec()\n",
        "    embedding_dict = {}\n",
        "    embedded_songs = []\n",
        "    words_not_known_to_word2vec=[]\n",
        "\n",
        "    for song in clean_lyrics_list_of_lists:\n",
        "      embedded_song = []\n",
        "      for word in song:\n",
        "        if word in embedding_dict.keys():\n",
        "          embedded_song.append(embedding_dict[word])\n",
        "          continue\n",
        "        else:\n",
        "          try:\n",
        "            embedded_word = word2vec.get_vector(word)\n",
        "            embedding_dict[word] = embedded_word\n",
        "            embedded_song.append(embedded_word)\n",
        "          except:\n",
        "            words_not_known_to_word2vec.append(word)\n",
        "            continue\n",
        "      embedded_songs.append(embedded_song)\n",
        "\n",
        "    pd.to_pickle(embedding_dict ,filepath_or_buffer=os.path.join(path_to_assingment_folder , \"embedding_dict\"))\n",
        "    pd.to_pickle(embedded_songs ,filepath_or_buffer=os.path.join(path_to_assingment_folder , \"embedded_lyrics\"))\n",
        "    pd.to_pickle(words_not_known_to_word2vec ,filepath_or_buffer=os.path.join(path_to_assingment_folder , \"words_not_known_to_word2vec\"))\n",
        "    \n",
        "  return embedding_dict, embedded_songs, words_not_known_to_word2vec\n",
        "\n",
        "def extract_features_from_midi(midi_files):\n",
        "  '''\n",
        "  DESCRIPTION: this function get list of midi files (one for eats song) and generates a list of features for each song.\n",
        "  PARAMETERS:\n",
        "  :param midi_files: list if midi files [midi_file,midi_file,...]\n",
        "  :return: list of midi features [[features for midi file #1],[features for midi file #2],...]\n",
        "  '''\n",
        "  midi_features = []\n",
        "  for midi_file in midi_files:\n",
        "    midi_features.append(extract_vector(midi_file))\n",
        "  return midi_features\n",
        "\n",
        "def midi_vector_naive(midi):\n",
        "  tempo = np.array([midi.estimate_tempo()])\n",
        "  chroma = np.array(midi.get_chroma().mean(-1))\n",
        "  piano_roll = np.array(midi.get_piano_roll().mean(-1))\n",
        "  num_of_instrument = np.array([len(midi.instruments)])\n",
        "  vector_list = [tempo,chroma,piano_roll,num_of_instrument]\n",
        "  midi_vector = np.concatenate(vector_list)\n",
        "  return np.float32(midi_vector)\n",
        "\n",
        "\n",
        "def extract_vector(midi_data):\n",
        "    # Load the MIDI file\n",
        "\n",
        "    \n",
        "    # Initialize an empty list to store the vector\n",
        "    eps = 1e-10\n",
        "    vector = []\n",
        "    \n",
        "    # Append the length of the time signature list\n",
        "    vector.append(len(midi_data.time_signature_changes))\n",
        "    \n",
        "    # Initialize an empty list to store the mean of the active rows and the average of the indices\n",
        "    programs_stats = [0]*100\n",
        "    # Iterate over each instrument in the MIDI file\n",
        "    for instrument in midi_data.instruments:\n",
        "        if instrument.program == 0 or instrument.program > 99:\n",
        "          continue\n",
        "        \n",
        "        # Extract the piano roll for the instrument\n",
        "        piano_roll = np.array(instrument.get_piano_roll())\n",
        "        # Get the indices of the 7 rows with the most values\n",
        "        most_active_rows = np.argsort(np.sum(piano_roll, axis=1))[-7:]\n",
        "\n",
        "        # Get the mean of the 7 most active rows\n",
        "        mean_active_rows = np.array(piano_roll[most_active_rows].mean(-1))\n",
        "\n",
        "        # Get the mean of the indices of the 7 most active rows\n",
        "        mean_active_rows_idx = np.array(most_active_rows).mean(-1)\n",
        "        programs_stats[instrument.program] = weight(mean_active_rows.mean(),mean_active_rows_idx)\n",
        "\n",
        "    programs_stats = [(value/(sum(programs_stats)+eps)) for value in programs_stats]\n",
        "    programs_stats[0] = len(midi_data.time_signature_changes)\n",
        "    # append the numpy to the vector\n",
        "    vector = np.array(programs_stats,dtype=np.float32)\n",
        "\n",
        "    return vector\n",
        "\n",
        "\n",
        "def weight(x, y ,alpha=0.8):\n",
        "  return (1-alpha)*x + alpha*y\n",
        "\n",
        "\n",
        "\n",
        "EMBEDDING_SIZE = 300\n",
        "PAD_VALUE = 0\n",
        "\n",
        "# PADDING - not used at the moment\n",
        "def add_padding(song, required_size):\n",
        "  '''\n",
        "  DESCRIPTION: this function adds padding in case we give a whole song as input to the network.\n",
        "  PARAMETERS:\n",
        "  :param song: song to add padding to\n",
        "  :param required_size: target size\n",
        "  :return: original song with padding equal to PAD_VALUE at the required size\n",
        "  '''\n",
        "  song_size = len(song)\n",
        "  pad_size = required_size - song_size\n",
        "  pad = [[PAD_VALUE for i in range(EMBEDDING_SIZE)] for i in range(pad_size)]\n",
        "  return song + pad\n",
        "\n",
        "# PADDING - not used at the moment\n",
        "def remove_padding(x):\n",
        "  '''\n",
        "  DESCRIPTION: this function remove padding for a single batch\n",
        "  PARAMETERS:\n",
        "  :param x: batch\n",
        "  :return: x without unnesesary padding\n",
        "  '''\n",
        "  pad = torch.tensor([0 for i in range(300)]).cuda()\n",
        "  max_len = 0\n",
        "  for seq in x:\n",
        "    for i, word in enumerate(seq):\n",
        "      if torch.all(pad.eq(word)):\n",
        "        if i + 1 > max_len:\n",
        "            max_len = i + 1\n",
        "        break\n",
        "  if max_len == 0:\n",
        "    max_len = x.shape[1]\n",
        "  return x[:,:max_len,:]\n",
        "\n",
        "def get_test_tokens(clean_lyrics_list_of_lists):\n",
        "  '''\n",
        "  DESCRIPTION: this function return the test tokens\n",
        "  PARAMETERS:\n",
        "  :param clean_lyrics_list_of_lists: list of the song lyrics\n",
        "  :return: list containing the first word in each song aka test token\n",
        "  '''\n",
        "  test_tokens = []\n",
        "  for song in clean_lyrics_list_of_lists:\n",
        "    test_tokens.append(song[0])\n",
        "  return test_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VRsZNMMa9qyo"
      },
      "outputs": [],
      "source": [
        "# Clean the lyrics\n",
        "clean_lyrics_list_of_lists_train = get_clean_lyrics_list_of_list(lyrics_train)\n",
        "clean_lyrics_list_of_lists_test = get_clean_lyrics_list_of_list(lyrics_test)\n",
        "\n",
        "# Generate embedding\n",
        "embedding_dict, embedded_lyrics_train, words_not_known_to_word2vec = get_embedding_dict_and_embedded_lyrics(clean_lyrics_list_of_lists_train)\n",
        "\n",
        "# Extract first word in test lyrcis as test token\n",
        "test_tokens = get_test_tokens(clean_lyrics_list_of_lists_test)\n",
        "\n",
        "# Extract midi features\n",
        "try:\n",
        "  midi_features_train = pd.read_pickle(os.path.join(path_to_assingment_folder ,\"midi_features_train_2\"))\n",
        "  midi_features_test = pd.read_pickle(os.path.join(path_to_assingment_folder ,\"midi_features_test_2\"))\n",
        "except:\n",
        "  midi_features_train = extract_features_from_midi(midis_train)\n",
        "  midi_features_test =  extract_features_from_midi(midis_test)\n",
        "  pd.to_pickle(midi_features_train ,filepath_or_buffer=os.path.join(path_to_assingment_folder , \"midi_features_train_2\"))\n",
        "  pd.to_pickle(midi_features_test ,filepath_or_buffer=os.path.join(path_to_assingment_folder , \"midi_features_test_2\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Dataset and Model"
      ],
      "metadata": {
        "id": "326amnuIT2Fc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-MJix8wj5UmH"
      },
      "outputs": [],
      "source": [
        "# Dataset helper function\n",
        "\n",
        "def get_sequences_sliding_window(embedded_lyrics, window_size, stride):\n",
        "  '''\n",
        "  DESCRIPTION: this function applies sliding window on song lyrics\n",
        "  PARAMETERS:\n",
        "  :param embedded_lyrics: song lyrics to generate sequences from\n",
        "  :param window_size: which window size to use when generating sequences of words\n",
        "  :return: list of sequences of according the given window size\n",
        "  '''\n",
        "  sequences = []\n",
        "  for song in embedded_lyrics:\n",
        "    sequences += [song[i:i+window_size] for i in range(0, len(song)-window_size+1, stride)]\n",
        "\n",
        "  return sequences\n",
        "\n",
        "def combine_word_embedding_with_midi_features(embedded_lyrics, midi_features):\n",
        "  '''\n",
        "  DESCRIPTION: this function combine the word embeddings with the midi features\n",
        "  PARAMETERS:\n",
        "  :param embedded_lyrics: embedded song lyrics\n",
        "  :param midi_features: list of midi_features where each object contains the features for the whole song\n",
        "  :return: embedded song lyrics concated with the midi_features\n",
        "  '''\n",
        "  embedded_lyrics_midi = []\n",
        "  for i, song in enumerate(embedded_lyrics):\n",
        "    song_with_midi = []\n",
        "    for word in song:\n",
        "      song_with_midi.append(np.concatenate((word, midi_features[i]), axis=None))\n",
        "    embedded_lyrics_midi.append(song_with_midi)\n",
        "  return embedded_lyrics_midi\n",
        "\n",
        "# Define the dataset class \n",
        "\n",
        "class LyricsDataset(Dataset):\n",
        "  def __init__(self, embedded_lyrics, sequence_len, window_size, stride , midi_features):\n",
        "    \n",
        "    # Combine word embeddings with midi features\n",
        "    embedded_lyrics_midi = combine_word_embedding_with_midi_features(embedded_lyrics, midi_features)\n",
        "\n",
        "    # Sequencing with sliding window\n",
        "    sequences = get_sequences_sliding_window(embedded_lyrics_midi, window_size=window_size, stride=stride)\n",
        "\n",
        "    self.data = torch.tensor(sequences)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.data[index]\n",
        "\n",
        "# Define the LSTM model \n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, input_size, embedding_dim, hidden_dim, num_layers, batch_size):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.properties = {'num_layers' : num_layers,\n",
        "                      'batch_size' : batch_size,\n",
        "                      'hidden_dim' : hidden_dim}\n",
        "    self.dropout = nn.Dropout(p=0.4)\n",
        "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, dropout=0.4)\n",
        "    self.fc = nn.Linear(hidden_dim, embedding_dim)\n",
        "  \n",
        "  def forward(self, x, h=None, c=None):\n",
        "    if h is None:\n",
        "      h = torch.zeros(self.properties['num_layers'],\n",
        "                      self.properties['batch_size'],\n",
        "                      self.properties['hidden_dim']).to(device)\n",
        "    if c is None:\n",
        "      c = torch.zeros(self.properties['num_layers'],\n",
        "                      self.properties['batch_size'],\n",
        "                      self.properties['hidden_dim']).to(device)\n",
        "    hidden = (h, c)\n",
        "\n",
        "    # Pass the embedded data through the LSTM layers\n",
        "    x, hidden = self.lstm(x, hidden)\n",
        "    x = self.dropout(x)\n",
        "    # Pass the output of the LSTM layers through the output layer\n",
        "    logits = self.fc(x)\n",
        "\n",
        "    return logits, hidden\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining our dataset object\n",
        "try:\n",
        "  lyrics_dataset = torch.load(os.path.join(path_to_assingment_folder, 'lyrics_dataset_2'))\n",
        "except:\n",
        "  lyrics_dataset = LyricsDataset(embedded_lyrics = embedded_lyrics_train, sequence_len=5, window_size=16, stride=1 , midi_features=midi_features_train)\n",
        "  torch.save(lyrics_dataset, os.path.join(path_to_assingment_folder, 'lyrics_dataset_2'))"
      ],
      "metadata": {
        "id": "H5Vosm3EB_qN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_lyrics_list_of_lists_train = []\n",
        "embedding_dict, embedded_lyrics_train, words_not_known_to_word2vec = get_embedding_dict_and_embedded_lyrics(clean_lyrics_list_of_lists_train)"
      ],
      "metadata": {
        "id": "hxSHCeKQschm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Predicting functions and helpers"
      ],
      "metadata": {
        "id": "x7Iqxi4lXA6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions for Train and Predict \n",
        "\n",
        "def most_similar_n(output, top_n, last_word):\n",
        "  '''\n",
        "  DESCRIPTION: this function return the top n similar words to the our put from the vocab via cosine similarity\n",
        "  PARAMETERS:\n",
        "  :param output: output of the model - vector of embedding size\n",
        "  :param top_n: number of top n words to return\n",
        "  :return: dictionary of top n similar words\n",
        "  '''\n",
        "  distance_measure = nn.CosineSimilarity(dim = 0)\n",
        "  similarity_dict = {}\n",
        "  for word, embedding in embedding_dict.items():\n",
        "      similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n",
        "  most_similar = dict(heapq.nlargest(top_n, similarity_dict.items(), key=itemgetter(1)))\n",
        "\n",
        "  # Dont predict the same word twice in a row\n",
        "  try:\n",
        "    most_similar.pop(last_word)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  return most_similar\n",
        "\n",
        "def min_max_normalization(similarity_dict):\n",
        "  '''\n",
        "  DESCRIPTION: this function applies minmax normalization\n",
        "  PARAMETERS:\n",
        "  :param similarity_dict: dictionary containign values to normalize\n",
        "  :return: orignial dictionary with same keys, values are the normalized values\n",
        "  '''\n",
        "  max_val = max(similarity_dict.values())\n",
        "  min_val = min(similarity_dict.values())\n",
        "  for key in similarity_dict.keys():\n",
        "    similarity_dict[key] = (similarity_dict[key] - min_val) / (max_val - min_val)\n",
        "  return similarity_dict\n",
        "\n",
        "def choose_word(normalized_distance_dict):\n",
        "  '''\n",
        "  DESCRIPTION: this function choses the next word out of the top n similar words in the dict\n",
        "  PARAMETERS:\n",
        "  :param probability_dict: dictionary containig the top n similar words and thier normalized distance\n",
        "  :return: the word chosen \n",
        "  '''\n",
        "\n",
        "  best_candidate = max(normalized_distance_dict.keys(), key=lambda x: normalized_distance_dict[x])\n",
        "  # Choose line seperator if its the best candidate - emiprically worked the best\n",
        "  if best_candidate == '&':\n",
        "    return '&'\n",
        "  # Get a list of the keys in the dictionary\n",
        "  keys = list(normalized_distance_dict.keys())\n",
        "  # Get a list of the values in the dictionary\n",
        "  values = list(normalized_distance_dict.values())\n",
        "  # Normalize the values so they add up to 1\n",
        "  values = [x/sum(values) for x in values]\n",
        "  # Choose a random value between 0 and 1\n",
        "  r = random.random()\n",
        "  # Find the word that corresponds to the random value\n",
        "  for i, v in enumerate(values):\n",
        "      r -= v\n",
        "      if r <= 0:\n",
        "          return keys[i]\n",
        "\n",
        "# Implementation of train and predict functions\n",
        "\n",
        "def train(model, dataloader_train, dataloader_validation, optimizer, criterion, num_epochs):\n",
        "  '''\n",
        "  DESCRIPTION: this function trains the model\n",
        "  '''\n",
        "  model.train()\n",
        "  \n",
        "  total_loss = 0\n",
        "  losses_train = []\n",
        "  losses_validation = []\n",
        "  epoch_times = []\n",
        "  for epoch in range(num_epochs):\n",
        "    print('='*50)\n",
        "    print('Epoch Number: ',epoch)\n",
        "    start = time.time()\n",
        "    epoch_losses_train = []\n",
        "    epoch_losses_validation = []\n",
        "    for i, batch in enumerate(dataloader_train):\n",
        "        batch = batch.to(device)\n",
        "        x = batch[...,:-1, :].to(device)\n",
        "        y = batch[...,1:, :].to(device)\n",
        "        output,_ = model(x)\n",
        "        word_loss = criterion(output.transpose(-1, -2), y[:, :, :300].transpose(-1, -2))\n",
        "        sequence_loss = sum(word_loss)\n",
        "        batch_loss = sum(sequence_loss)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Backward pass\n",
        "        batch_loss.backward()\n",
        "        \n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_losses_train.append(batch_loss)\n",
        "    with torch.no_grad():\n",
        "      for batch in dataloader_validation:\n",
        "        x = batch[...,:-1, :].to(device)\n",
        "        y = batch[...,1:, :].to(device)\n",
        "        output,_ = model(x)\n",
        "        word_loss = criterion(output.transpose(-1, -2), y[:, :, :300].transpose(-1, -2))\n",
        "        sequence_loss = sum(word_loss)\n",
        "        batch_loss = sum(sequence_loss)\n",
        "        epoch_losses_validation.append(batch_loss)\n",
        "\n",
        "    end = time.time()\n",
        "    epoch_time = end-start\n",
        "    avg_loss_train = sum(epoch_losses_train)/len(epoch_losses_train)\n",
        "    print(\"Average Loss Train: \" + str(avg_loss_train)) \n",
        "    avg_loss_validation = sum(epoch_losses_validation)/len(epoch_losses_validation)\n",
        "    print(\"Average Loss Validation: \" + str(avg_loss_validation)) \n",
        "    print(\"Epoch time: \" + str(epoch_time))\n",
        "    # Stopping criterion - if change in avg loss is less than 1, empirically afterwards we saw overfit in the results\n",
        "    if epoch > 5:\n",
        "      if avg_loss_validation > losses_validation[-1]*0.99:\n",
        "        return losses_train, losses_validation, epoch_times\n",
        "\n",
        "    losses_train.append(avg_loss_train)\n",
        "    losses_validation.append(avg_loss_validation)\n",
        "    epoch_times.append(epoch_time)\n",
        "\n",
        "  return losses_train, losses_validation, epoch_times\n",
        "\n",
        "def predict(model, first_word, device, song_size, midi_features):\n",
        "  '''\n",
        "  DESCRIPTION: this function generate a song accordig to given word and midi file\n",
        "  PARAMETERS:\n",
        "  :param model: trained model to use\n",
        "  :param first word: first word to predict with\n",
        "  :param device: device to calculate on - cuda, cpu \n",
        "  :param song_size: size of the song to be generated - number of words\n",
        "  :return: the word chosen \n",
        "  '''\n",
        "  model.eval()\n",
        "\n",
        "  # A list to keep all the words the model returned\n",
        "  words = [first_word]\n",
        "\n",
        "  # initializing the first hidden state (usually to zeros)\n",
        "  h = torch.zeros(model.properties['num_layers'],\n",
        "                  1,\n",
        "                  model.properties['hidden_dim']).to(device)\n",
        "\n",
        "  c = torch.zeros(model.properties['num_layers'], \n",
        "                  1,\n",
        "                  model.properties['hidden_dim']).to(device)\n",
        "\n",
        "  #x = torch.tensor([[embedding_dict[first_word]]]).to(device)\n",
        "  x = torch.tensor([[np.concatenate((embedding_dict[first_word], midi_features), axis=None)]]).to(device) \n",
        "\n",
        "  # iterating for the amount of tokens we want to genereate\n",
        "  next_word = \"\"\n",
        "  for i in range(0, song_size):\n",
        "\n",
        "      # predict the next token\n",
        "      # y_pred is the output of the linear layer which was fed the hidden state\n",
        "      output, (h, c) = model(x, h, c)\n",
        "\n",
        "      # y is shape (1, sequence length, embedding dim)\n",
        "      # so we only want the last token to decide what is the next\n",
        "      # so y[0] is shape (sequence length, embedding dim)\n",
        "      # and y[0][-1] is shape (embedding dim, ) and is specifically the last token\n",
        "      last_word = output[0][-1]\n",
        "      similarity_dict = most_similar_n(last_word, top_n = 5, last_word = next_word)\n",
        "      probability_dict = min_max_normalization(similarity_dict)\n",
        "      next_word = choose_word(probability_dict)\n",
        "      words.append(next_word)\n",
        "      x = torch.tensor([[np.concatenate((embedding_dict[next_word], midi_features), axis=None)]]).to(device)\n",
        "      #x = torch.tensor([[embedding_dict[next_word] + midi_features]]).to(device) remove comment to use midi features\n",
        "\n",
        "  return words"
      ],
      "metadata": {
        "id": "RWa6BnxyaGgT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "F5bwd6lwPlTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model, objective function, batch size, optimizer.\n",
        "\n",
        "vocab_size = len(embedding_dict)\n",
        "embedding_dim = 300 \n",
        "hidden_dim = 512  # Hyper Parameter\n",
        "num_layers = 1 # Hyper Parameter\n",
        "batch_size = 20\n",
        "input_size = len(lyrics_dataset[0][0])\n",
        "model = LSTM(vocab_size=vocab_size,input_size = input_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim, num_layers=num_layers, batch_size= batch_size).to(device)\n",
        "\n",
        "\n",
        "# splitting train data into train and validation.\n",
        "\n",
        "train_size = int(0.9 * len(lyrics_dataset))\n",
        "validation_size = len(lyrics_dataset) - train_size\n",
        "train_dataset, validation_dataset = random_split(lyrics_dataset, [train_size, validation_size])\n"
      ],
      "metadata": {
        "id": "bhyFmvT5OFtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25efd4c-82fe-4bfb-be44-3dc35f9d6bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "\n",
        "num_epochs = 20\n",
        "dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
        "dataloader_validation = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion_cs = nn.CosineSimilarity(dim=1)\n",
        "criterion = lambda x, y: 1- criterion_cs(x, y)\n",
        "losses_train, losses_validation, epoch_times = train(model=model, dataloader_train=dataloader_train, dataloader_validation=dataloader_validation, optimizer=optimizer, criterion=criterion, num_epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd-vxV6UnKac",
        "outputId": "c6fd067c-8650-43a2-c38d-7195467116bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Epoch Number:  0\n",
            "Average Loss Train: tensor(139.9747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Average Loss Validation: tensor(126.8950, device='cuda:0')\n",
            "Epoch time: 31.869155645370483\n",
            "==================================================\n",
            "Epoch Number:  1\n",
            "Average Loss Train: tensor(119.5864, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Average Loss Validation: tensor(114.7349, device='cuda:0')\n",
            "Epoch time: 30.806077480316162\n",
            "==================================================\n",
            "Epoch Number:  2\n",
            "Average Loss Train: tensor(110.3448, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Average Loss Validation: tensor(108.5517, device='cuda:0')\n",
            "Epoch time: 30.701199769973755\n",
            "==================================================\n",
            "Epoch Number:  3\n",
            "Average Loss Train: tensor(104.9108, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Average Loss Validation: tensor(104.2102, device='cuda:0')\n",
            "Epoch time: 30.779974222183228\n",
            "==================================================\n",
            "Epoch Number:  4\n",
            "Average Loss Train: tensor(101.1212, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Average Loss Validation: tensor(101.2739, device='cuda:0')\n",
            "Epoch time: 30.684524297714233\n",
            "==================================================\n",
            "Epoch Number:  5\n",
            "Average Loss Train: tensor(98.2250, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Average Loss Validation: tensor(98.9647, device='cuda:0')\n",
            "Epoch time: 30.662400722503662\n",
            "==================================================\n",
            "Epoch Number:  6\n",
            "Average Loss Train: tensor(95.9535, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Average Loss Validation: tensor(96.9729, device='cuda:0')\n",
            "Epoch time: 30.927481412887573\n",
            "==================================================\n",
            "Epoch Number:  7\n",
            "Average Loss Train: tensor(94.0982, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Average Loss Validation: tensor(95.3912, device='cuda:0')\n",
            "Epoch time: 30.54337739944458\n",
            "==================================================\n",
            "Epoch Number:  8\n",
            "Average Loss Train: tensor(92.5290, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Average Loss Validation: tensor(93.9691, device='cuda:0')\n",
            "Epoch time: 30.772114276885986\n",
            "==================================================\n",
            "Epoch Number:  9\n",
            "Average Loss Train: tensor(91.1495, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Average Loss Validation: tensor(92.8418, device='cuda:0')\n",
            "Epoch time: 30.673850059509277\n",
            "==================================================\n",
            "Epoch Number:  10\n",
            "Average Loss Train: tensor(89.9669, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Average Loss Validation: tensor(91.8160, device='cuda:0')\n",
            "Epoch time: 30.907461643218994\n",
            "==================================================\n",
            "Epoch Number:  11\n",
            "Average Loss Train: tensor(88.8993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Average Loss Validation: tensor(91.0244, device='cuda:0')\n",
            "Epoch time: 30.66081404685974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"training time total: \",sum(epoch_times), 'seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXBY8z1PlCUJ",
        "outputId": "dc91ed78-906f-4ec4-f32c-76fd1e6b8302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training time total:  339.32761693000793 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensorboard generation\n",
        "writer = SummaryWriter(log_dir=\"/content/midi_expert\")\n",
        "for i in range(len(losses_train)):\n",
        "    writer.add_scalars('losses', {'train':losses_train[i],\n",
        "                                    'validation':losses_validation[i]}\n",
        "                                    , i)"
      ],
      "metadata": {
        "id": "Rp3JFAf-QBYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writer.close()\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "84MZtmW9i5uh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# midi expert\n",
        "%tensorboard --logdir=/content/midi_expert"
      ],
      "metadata": {
        "id": "UllCszE6vDbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# midi naive\n",
        "%tensorboard --logdir=/content/midi_naive"
      ],
      "metadata": {
        "id": "Uc1uTOr9it5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only words\n",
        "%tensorboard --logdir=/content/runs"
      ],
      "metadata": {
        "id": "Of_qbVqqA4d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting and Reults"
      ],
      "metadata": {
        "id": "T2syfZAzznJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kVWolGieYqFi"
      },
      "outputs": [],
      "source": [
        "def sing_motherfucker(words):\n",
        "  joined_words = ' '.join(words)\n",
        "  sentences = joined_words.split('&')\n",
        "  for sentence in sentences:\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(embedding_dict)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 512  # Hyper Parameter\n",
        "num_layers = 1 # Hyper Parameter\n",
        "batch_size = 64\n",
        "input_size = len(lyrics_dataset[0][0])\n",
        "model = LSTM(vocab_size=vocab_size,input_size=input_size , embedding_dim=embedding_dim, hidden_dim=hidden_dim, num_layers=num_layers, batch_size= batch_size).to(device)\n",
        "model.load_state_dict(torch.load(os.path.join(path_to_assingment_folder, 'model_12ep_final_midi_expert')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBuLlj2_-gG3",
        "outputId": "40a76d9d-b43e-4c64-94a0-b33a3eff7780"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Naive Midi\")\n",
        "print(\"Word : time\")\n",
        "print(\"Song : Eternal Flame\")\n",
        "words = predict(model, 'time', device, song_size=100, midi_features= midi_features_test[0])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n925QmA2Z_4Y",
        "outputId": "e5582117-f8ac-46af-ffa4-34a952db9929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Naive Midi\n",
            "Word : time\n",
            "Song : Eternal Flame\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time \n",
            " anyway just not do \n",
            " do me anyway really do \n",
            " anyway just do me \n",
            " just do you think do you \n",
            " anyway do you want me \n",
            " anyway just do \n",
            " anyway do you do you want you \n",
            " anyway do just do \n",
            " just do you want me do \n",
            " just do you do you do \n",
            " anyway do you do you do \n",
            " just do you want you do \n",
            " just do you know do you \n",
            " do me do you do me \n",
            " anyway you know you do \n",
            " do you want do really\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Naive Midi\")\n",
        "print(\"Word : time\")\n",
        "print(\"Song : Honesty\")\n",
        "words = predict(model, 'time', device, song_size=100, midi_features= midi_features_test[1])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bZClPxnHlnz",
        "outputId": "ff804275-22ca-4e93-ddee-e4f3076c3bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Naive Midi\n",
            "Word : time\n",
            "Song : Honesty\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time \n",
            " anyway just do you \n",
            " anyway just do you \n",
            " just do you do you do \n",
            " anyway do you do me \n",
            " just do you know \n",
            " anyway just do you \n",
            " so anyway you do me \n",
            " just do you want you do \n",
            " just do you think do you \n",
            " just do you want you do \n",
            " just do you want do you want you know \n",
            " anyway you know you think do me \n",
            " you want you do me \n",
            " just do you want you do \n",
            " just want you want you \n",
            " you do\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Naive Midi\")\n",
        "print(\"Word : time\")\n",
        "print(\"Song : Loveful\")\n",
        "words = predict(model, 'time', device, song_size=100, midi_features= midi_features_test[2])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "id": "TeVPIPOQHoE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Naive Midi\")\n",
        "print(\"Word : time\")\n",
        "print(\"Song : Barbie Girl\")\n",
        "words = predict(model, 'time', device, song_size=100, midi_features= midi_features_test[3])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "id": "Oq6vLCrZHrU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Naive Midi\")\n",
        "print(\"Word : time\")\n",
        "print(\"Song : All the small things\")\n",
        "words = predict(model, 'time', device, song_size=100, midi_features= midi_features_test[4])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "id": "CXhgZuKdHuBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5"
      ],
      "metadata": {
        "id": "nzGvrDiMJRJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Expert Midi\")\n",
        "print(\"Word : time\")\n",
        "print(\"Song : Eternal Flame\")\n",
        "words = predict(model, 'time', device, song_size=100, midi_features= midi_features_test[0])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "id": "L0dnWMPAHxUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "653fbed0-247c-4f96-aa9b-300632f27ed2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Expert Midi\n",
            "Word : time\n",
            "Song : Eternal Flame\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dd0cde0938c4>:153: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  x = torch.tensor([[np.concatenate((embedding_dict[first_word], midi_features), axis=None)]]).to(device)\n",
            "<ipython-input-18-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time \n",
            " i dont think it just go in really do you know \n",
            " do hey do what you do i want \n",
            " i got my way ive been there is \n",
            " i dont want never do \n",
            " do yourself do really do thats what we \n",
            " cant anyway it in my mind can do your love \n",
            " i came here just in myself \n",
            " i just want be that you are \n",
            " your love should i do \n",
            " i do love do it \n",
            " know you can it go off \n",
            " anyway just the this just be so you can\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Expert Midi\")\n",
        "print(\"Word : time\")\n",
        "print(\"Song : Honesty\")\n",
        "words = predict(model, 'time', device, song_size=100, midi_features= midi_features_test[1])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "id": "rqAAIOOpH4g6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c22396-c107-4859-c444-5e1dbbe313bd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Expert Midi\n",
            "Word : time\n",
            "Song : Honesty\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time \n",
            " i do you do \n",
            " anyway what cant tell is \n",
            " what we can know why be do \n",
            " you know that i do \n",
            " i know you know \n",
            " hey oh \n",
            " oh hey \n",
            " oh hey \n",
            " just think your love \n",
            " so anyways do you know you do \n",
            " i do know what dont do me thats gotta say \n",
            " he wanted get in his life \n",
            " just so yeah \n",
            " im oh girl \n",
            " i know i know \n",
            " but i do you know you got \n",
            " i do it just do the just do\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Expert Midi\")\n",
        "print(\"Word : time\")\n",
        "print(\"Song : Loveful\")\n",
        "words = predict(model, 'time', device, song_size=100, midi_features= midi_features_test[2])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "id": "iCd7GsMUH_kV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb1697a-b886-450a-f96a-7060387d0fab"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Expert Midi\n",
            "Word : time\n",
            "Song : Loveful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time \n",
            " i do know i think \n",
            " i know your love whoah \n",
            " im gonna get on haha \n",
            " cause im the just do \n",
            " do anyway do cant tell me \n",
            " dont think know youre know yourself \n",
            " anyway so dont do the shit do you think \n",
            " i know thats just im gonna get down \n",
            " hey dont wanna get just boy \n",
            " thats gonna be good mama \n",
            " hey cuz gonna know just gonna do \n",
            " do me do dont do my love do \n",
            " oh hey oh \n",
            " oh yeah \n",
            " oh ya know what dont\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Expert Midi\")\n",
        "print(\"Word : time\")\n",
        "print(\"Song : Barbie Girl\")\n",
        "words = predict(model, 'time', device, song_size=100, midi_features= midi_features_test[3])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "id": "tLzAwD3xIoNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12e9b452-5990-4042-ef99-7b639212a09e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Expert Midi\n",
            "Word : time\n",
            "Song : Barbie Girl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time \n",
            " just do it just want do \n",
            " know it do \n",
            " do you do it really \n",
            " so do your just do really think \n",
            " really know it just so not \n",
            " anyway i think \n",
            " not do hey anyway \n",
            " thats just the really hey \n",
            " i dont want you do \n",
            " hey oh thats just really \n",
            " what thats do \n",
            " you want it just so do \n",
            " oh haha oh \n",
            " oh i want your really want \n",
            " just do it just do it \n",
            " really want it all do you do it just \n",
            " go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Expert Midi\")\n",
        "print(\"Word : time\")\n",
        "print(\"Song : All the small things\")\n",
        "words = predict(model, 'time', device, song_size=100, midi_features= midi_features_test[4])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "id": "FRuAZzn2IsQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e333736-ecb3-4873-cbd0-cd9f6ec448ae"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Expert Midi\n",
            "Word : time\n",
            "Song : All the small things\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time \n",
            " thats how i was \n",
            " just got just do not that you should get me \n",
            " just the do just do just anyway do your just want go it \n",
            " go it go just get just up just you \n",
            " just want it just it just do it all just really think that we can just come just then just want be \n",
            " so do just the really do you think you know \n",
            " what i want do \n",
            " thats really thats what just really do \n",
            " just one just do it all out really know it just do\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10"
      ],
      "metadata": {
        "id": "_V96X_5CJLiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Naive Midi\")\n",
        "print(\"Word : Party\")\n",
        "print(\"Song : Eternal Flame\")\n",
        "words = predict(model, 'party', device, song_size=100, midi_features= midi_features_test[0])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "id": "FOsolMbxI37q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Naive Midi\")\n",
        "print(\"Word : Party\")\n",
        "print(\"Song : Honesty\")\n",
        "words = predict(model, 'party', device, song_size=100, midi_features= midi_features_test[1])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "id": "sBenv70pJMvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Naive Midi\")\n",
        "print(\"Word : Party\")\n",
        "print(\"Song : Loveful\")\n",
        "words = predict(model, 'party', device, song_size=100, midi_features= midi_features_test[2])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "id": "DvSkFyhXJMkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Naive Midi\")\n",
        "print(\"Word : Party\")\n",
        "print(\"Song : Barbie Girl\")\n",
        "words = predict(model, 'party', device, song_size=100, midi_features= midi_features_test[3])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "id": "EEMpV3yXJMUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Naive Midi\")\n",
        "print(\"Word : Party\")\n",
        "print(\"Song :  All The Small Things\")\n",
        "words = predict(model, 'party', device, song_size=100, midi_features= midi_features_test[4])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "id": "qJlPnYbcJpB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15"
      ],
      "metadata": {
        "id": "hQUtceINJtAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Expert Midi\")\n",
        "print(\"Word : Party\")\n",
        "print(\"Song : Eternal Flame\")\n",
        "words = predict(model, 'party', device, song_size=100, midi_features= midi_features_test[0])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "id": "1RjMlyD0Jt9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f723b9-59ae-4f81-e581-d15daf40e12c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Expert Midi\n",
            "Word : Party\n",
            "Song : Eternal Flame\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "party \n",
            " just really just really do \n",
            " what you do im not anyway \n",
            " do thats just what we do \n",
            " alright just gonna be gotta get just \n",
            " this gotta be just one just you \n",
            " we can do anyway \n",
            " they do me do dont know you want it cant go just \n",
            " but ya gotta get the just get that dont be anyway \n",
            " you do really want know how do it \n",
            " thats what i do \n",
            " i think love where the kid go out \n",
            " go going just go out \n",
            " hey oh yeah oh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Expert Midi\")\n",
        "print(\"Word : Party\")\n",
        "print(\"Song : Honesty\")\n",
        "words = predict(model, 'party', device, song_size=100, midi_features= midi_features_test[1])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "id": "51Z-WAnMJ37J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1865115-a527-45d2-a42c-38188f349f74"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Expert Midi\n",
            "Word : Party\n",
            "Song : Honesty\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "party \n",
            " we go on now \n",
            " if i do yourself do i do you do \n",
            " i know you want know i want yourself \n",
            " so just do \n",
            " just what you not know what when ive get you \n",
            " anyway it so \n",
            " so i go \n",
            " just do \n",
            " just what i do \n",
            " i know do what i do \n",
            " i think you want do \n",
            " i know you think that just not just do \n",
            " so anyway the day thats hey it \n",
            " anyway hey \n",
            " hey oh hahaha yeah \n",
            " yeah hey \n",
            " so anyway\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Expert Midi\")\n",
        "print(\"Word : Party\")\n",
        "print(\"Song : Loveful\")\n",
        "words = predict(model, 'party', device, song_size=100, midi_features= midi_features_test[2])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB5JA4KsOEVI",
        "outputId": "3e0ae10a-49a6-42af-9ef0-379364dd8922"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Expert Midi\n",
            "Word : Party\n",
            "Song : Loveful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "party just \n",
            " do you tell me how \n",
            " my love is just like \n",
            " do your love is really like \n",
            " do you know your soul love \n",
            " i really do \n",
            " go it just go out just when i get out \n",
            " thats gotta do my love \n",
            " just like the shit wanna just hey haha \n",
            " her sweety seems just do \n",
            " just do hey oh hey oh cuz ya \n",
            " anyway hey dont know anyway hey \n",
            " ohhh my oh hey thats hey i gotta \n",
            " anyway hey thats hey anyway \n",
            " dont think anyway you know\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Expert Midi\")\n",
        "print(\"Word : Party\")\n",
        "print(\"Song : Barbie Girl\")\n",
        "words = predict(model, 'party', device, song_size=100, midi_features= midi_features_test[3])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWADgsO_OIRd",
        "outputId": "c12d1743-78d5-4313-d837-7b04db17b592"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Expert Midi\n",
            "Word : Party\n",
            "Song : Barbie Girl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "party \n",
            " maybe just do you think yeah \n",
            " anyways hey yeah \n",
            " yeah hey \n",
            " so i can \n",
            " know thats the so really do that gotta do it \n",
            " but you cant do it \n",
            " it can know it just not do \n",
            " just do you think yeah \n",
            " but i want do it \n",
            " just want just really do what want do \n",
            " do you do it really \n",
            " if your just do you do \n",
            " oh but hey yeah hey oh uh \n",
            " anyway just do \n",
            " know what do it do \n",
            " so just want it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Expert Midi\")\n",
        "print(\"Word : Party\")\n",
        "print(\"Song : All The Small Things\")\n",
        "words = predict(model, 'party', device, song_size=100, midi_features= midi_features_test[4])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFuC2qGKOY4K",
        "outputId": "1476bd19-fafe-4ec2-81bd-691fb8b7f20b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Expert Midi\n",
            "Word : Party\n",
            "Song : All The Small Things\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "party so \n",
            " but thats just this just what do that know \n",
            " the just me do it all the way \n",
            " so do just the just go on the it \n",
            " hey oh hey oh hey oh \n",
            " oh hey yeah \n",
            " just really do it really do it just do just the really your just do you know \n",
            " it could be do it can make \n",
            " anyway just do your just what i do \n",
            " hey anyway just wanna go just really want do \n",
            " its just what do it do \n",
            " it seems that we would be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 20"
      ],
      "metadata": {
        "id": "gQg4dwckOeoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Expert Midi\")\n",
        "print(\"Word : Car\")\n",
        "print(\"Song : Eternal Flame\")\n",
        "words = predict(model, 'car', device, song_size=100, midi_features= midi_features_test[0])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2EUYez7Oozu",
        "outputId": "d9e33efe-e436-4137-d9d6-1ad261d4179a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Expert Midi\n",
            "Word : Car\n",
            "Song : Eternal Flame\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car \n",
            " anyway so do something \n",
            " i want yourself for just i know you think thats really do \n",
            " do that yourself wanna do \n",
            " anyway not do \n",
            " do yourself just do i know \n",
            " anyway really want think that we want \n",
            " we need you anyway \n",
            " i do want you do \n",
            " anyway hey i wanna love you hey mama \n",
            " just so gotta get anyway \n",
            " that you cant want the day that we get \n",
            " i love you \n",
            " not that really wanna be really want \n",
            " come just go in loves just cant be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Expert Midi\")\n",
        "print(\"Word : Car\")\n",
        "print(\"Song : Honesty\")\n",
        "words = predict(model, 'car', device, song_size=100, midi_features= midi_features_test[1])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teReKeFbOqvo",
        "outputId": "c61120c3-babc-4553-a341-340e56b3200c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Expert Midi\n",
            "Word : Car\n",
            "Song : Honesty\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car \n",
            " just do you know you do \n",
            " thats how ive had you \n",
            " just so your daddy \n",
            " do you know what should i do \n",
            " dont you think i got really do yourself \n",
            " but when i go just anyway \n",
            " anyway i want know i know yourself do just really do \n",
            " it comes just really \n",
            " if i love you so \n",
            " not so not so anyway \n",
            " so dont you know you do \n",
            " thats what i know \n",
            " so anyway be \n",
            " if your eyes can see what they are \n",
            " anyway thats just\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Expert Midi\")\n",
        "print(\"Word : Car\")\n",
        "print(\"Song : Loveful\")\n",
        "words = predict(model, 'car', device, song_size=100, midi_features= midi_features_test[2])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC204_KHOs5k",
        "outputId": "8cbe14ea-62d3-45c0-e0f8-101a2b7840b8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Expert Midi\n",
            "Word : Car\n",
            "Song : Loveful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car \n",
            " just get just hey \n",
            " hey oh hey oh ya oh \n",
            " oh hey oh \n",
            " oh hey oh \n",
            " oh yeah oh \n",
            " ya think just gotta do just like \n",
            " just really wanna dont wanna anyway \n",
            " i just want get back on the way hey \n",
            " dont wanna know youre going back \n",
            " anyway come on hey yeah gotta do just do \n",
            " just do just wanna go down like all hey \n",
            " just really wanna you wanna do \n",
            " do you want come down \n",
            " but cuz you do \n",
            " hey yeah i know its\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Expert Midi\")\n",
        "print(\"Word : Car\")\n",
        "print(\"Song : Barbie Girl\")\n",
        "words = predict(model, 'car', device, song_size=100, midi_features= midi_features_test[3])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwRt4bXhOvTZ",
        "outputId": "c9b12f7f-add0-413a-d39d-1c0994c294a2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Expert Midi\n",
            "Word : Car\n",
            "Song : Barbie Girl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car \n",
            " its anyway just what just it \n",
            " its really know if i do \n",
            " i know you know that you want it \n",
            " but if yourself anyway do it \n",
            " anyway you do \n",
            " do me know what you know \n",
            " thats really do it really \n",
            " what do your do anyway know \n",
            " do just do it just do it \n",
            " anyway \n",
            " so i can do it just \n",
            " do \n",
            " so you should not it go \n",
            " but i can do something do \n",
            " do me really want it just go \n",
            " so just do it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model : Expert Midi\")\n",
        "print(\"Word : Car\")\n",
        "print(\"Song : All The Small Things\")\n",
        "words = predict(model, 'car', device, song_size=100, midi_features= midi_features_test[4])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvyjaANFOw70",
        "outputId": "03a87adc-51be-4033-ce9b-fe96cbcdf538"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model : Expert Midi\n",
            "Word : Car\n",
            "Song : All The Small Things\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dd0cde0938c4>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car \n",
            " it is anyway hey im gotta get just really just gotta do it \n",
            " but the just what you know \n",
            " what can know is do \n",
            " your boy seems just you just hey \n",
            " just come on just it just do it just really do \n",
            " just do we anyway not you \n",
            " anyway dont do my really what you know \n",
            " anyway just do it all just really do just the maybe \n",
            " hey really want do all just really want just do it \n",
            " if somebody think just what do it do \n",
            " do it all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RW7HCKH7PUMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = predict(model, 'car', device, song_size=100, midi_features=midi_features_test[1])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDJQImPg3lzy",
        "outputId": "5420f65a-c6c4-419e-d733-ec9bfbe5ab20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-33764f2b80aa>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dear \n",
            " thats just what hey anyway you \n",
            " but it really think it just no so \n",
            " no you gotta know \n",
            " but it do not it \n",
            " but your hands just got guess you \n",
            " do you know \n",
            " what can i do \n",
            " just do your kinda crazy \n",
            " really do know what its like crazy when \n",
            " so that you got me do you want know \n",
            " if its love could get \n",
            " it really \n",
            " but i know that i be hey \n",
            " if you wanna are just hey really want really do \n",
            " just gotta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word2vec doesnt know hiya and barbie, inference in girl instead\n",
        "words = predict(model, 'girl', device, song_size=100, midi_features=midi_features_test[2])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GDDuWfucTyt",
        "outputId": "094bd231-5df9-4ada-bea4-026958112746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-33764f2b80aa>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "girl \n",
            " hey oh hey oh yeah \n",
            " just do it just do it just do you want come down \n",
            " let me know we let just do \n",
            " really want do \n",
            " but i know that i cant want go \n",
            " im gotta go here \n",
            " just do it anyway you want \n",
            " just do it just anyway \n",
            " on that night \n",
            " the one so sweet love me for \n",
            " my love for you hey \n",
            " if you do it you want it \n",
            " never you want something never do can know what loved you know \n",
            " i thought\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = predict(model, 'all', device, song_size=100, midi_features=midi_features_test[3])\n",
        "sing_motherfucker(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge4lyJzKcURH",
        "outputId": "2f1b1923-3e12-421f-f14d-6e96abf36877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-33764f2b80aa>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  similarity_dict[word] = distance_measure(torch.tensor(output).to(device), torch.tensor(embedding).to(device)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all just do \n",
            " then really know what your kid seems seein is up \n",
            " oh you need me so i want you \n",
            " hey maybe thats what it is \n",
            " because it you know \n",
            " if i can even just me \n",
            " anyway dont do it \n",
            " when you want get it \n",
            " but dont you think that it becomes so bad \n",
            " we not can know really wont come me \n",
            " but the just you just for my life \n",
            " oh hey oh hey oh \n",
            " oh uh oh yeah hey yeah \n",
            " yeah hey oh yeah \n",
            " the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIxQo_UduFPl"
      },
      "outputs": [],
      "source": [
        "# Saving Models after train\n",
        "torch.save(model.state_dict(), os.path.join(path_to_assingment_folder, 'model_12ep_final_midi_expert')) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.to_pickle(losses_train ,filepath_or_buffer=os.path.join(path_to_assingment_folder , \"losses_train_midi_expert\"))\n",
        "pd.to_pickle(losses_validation ,filepath_or_buffer=os.path.join(path_to_assingment_folder , \"losses_validation_midi_expert\"))\n",
        "pd.to_pickle(epoch_times ,filepath_or_buffer=os.path.join(path_to_assingment_folder , \"epoch_times_midi_expert\"))"
      ],
      "metadata": {
        "id": "2dCqAgkzgciY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "UcgNiniEWPIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Define the LSTM model bidirect\n",
        "\n",
        "# class LSTM(nn.Module):\n",
        "#   def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, batch_size):\n",
        "#     super(LSTM, self).__init__()\n",
        "#     self.properties = {'num_layers' : num_layers,\n",
        "#                       'batch_size' : batch_size,\n",
        "#                       'hidden_dim' : hidden_dim}\n",
        "#     # Define the embedding layer to convert words into numerical representations\n",
        "#     #self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "#     # Define the LSTM layers\n",
        "#     self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, dropout=0.6, bidirectional=True)\n",
        "#     # Define the output layer\n",
        "#     #self.fc = nn.Linear(vocab_size, embedding_dim)\n",
        "#     self.fc = nn.Linear(hidden_dim*2, embedding_dim)\n",
        "  \n",
        "#   def forward(self, x, h=None, c=None):\n",
        "\n",
        "#     if h is None:\n",
        "#       h = torch.zeros(self.properties['num_layers']*2,\n",
        "#                       self.properties['batch_size'],\n",
        "#                       self.properties['hidden_dim']).to(device)#.double().to(device)\n",
        "#     if c is None:\n",
        "#       c = torch.zeros(self.properties['num_layers']*2,\n",
        "#                       self.properties['batch_size'],\n",
        "#                       self.properties['hidden_dim']).to(device)#.double().to(device)\n",
        "    \n",
        "#     hidden = (h, c)#.to(device))\n",
        "#     # Pass the input data through the embedding layer\n",
        "#     #x = self.embedding(x)\n",
        "#     # Pass the embedded data through the LSTM layers\n",
        "#     x, hidden = self.lstm(x, hidden)\n",
        "#     # Pass the output of the LSTM layers through the output layer\n",
        "#     logits = self.fc(x)\n",
        "#     return logits, hidden"
      ],
      "metadata": {
        "id": "9SLApDsfWRX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset class for your lyrics data\n",
        "\n",
        "class LyricsDataset(Dataset):\n",
        "  def __init__(self, embedded_lyrics, sequence_len, window_size):\n",
        "    \n",
        "    # Sequencing with sliding window\n",
        "    sequences = get_sequences_sliding_window(embedded_lyrics, window_size=window_size)\n",
        "    # Padding & Sequencing\n",
        "    # max_len = len(max(embedded_lyrics, key=len))\n",
        "    # sequences = []\n",
        "    # for ind, song in enumerate(embedded_lyrics):\n",
        "        # num_sequences = len(song)//sequence_len\n",
        "        # for sequence in range(num_sequences):\n",
        "        #     start = sequence*sequence_len\n",
        "        #     end = (sequence+1)*sequence_len\n",
        "        #     sequences.append(song[start:end])\n",
        "        # embedded_lyrics[ind] = add_padding(song, max_len)\n",
        "\n",
        "\n",
        "    #embedded_lyrics_np = np.array([np.array(xi) for xi in embedded_lyrics])\n",
        "    #self.data = torch.from_numpy(embedded_lyrics_np).double()\n",
        "    self.data = torch.tensor(sequences)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.data[index]\n"
      ],
      "metadata": {
        "id": "pgrHviX6WawI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model \n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, batch_size):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.properties = {'num_layers' : num_layers,\n",
        "                      'batch_size' : batch_size,\n",
        "                      'hidden_dim' : hidden_dim}\n",
        "    # Define the embedding layer to convert words into numerical representations\n",
        "    #self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    # Define the LSTM layers\n",
        "    self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, dropout=0.6)\n",
        "    # Define the output layer\n",
        "    #self.fc = nn.Linear(vocab_size, embedding_dim)\n",
        "    self.fc = nn.Linear(hidden_dim, embedding_dim)\n",
        "  \n",
        "  def forward(self, x, h=None, c=None):\n",
        "\n",
        "    if h is None:\n",
        "      h = torch.zeros(self.properties['num_layers'],\n",
        "                      self.properties['batch_size'],\n",
        "                      self.properties['hidden_dim']).to(device)#.double().to(device)\n",
        "    if c is None:\n",
        "      c = torch.zeros(self.properties['num_layers'],\n",
        "                      self.properties['batch_size'],\n",
        "                      self.properties['hidden_dim']).to(device)#.double().to(device)\n",
        "    \n",
        "    hidden = (h, c)#.to(device))\n",
        "    # Pass the input data through the embedding layer\n",
        "    #x = self.embedding(x)\n",
        "    # Pass the embedded data through the LSTM layers\n",
        "    x, hidden = self.lstm(x, hidden)\n",
        "    # Pass the output of the LSTM layers through the output layer\n",
        "    logits = self.fc(x)\n",
        "    return logits, hidden"
      ],
      "metadata": {
        "id": "YtdEUnO9WlSb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bAzbpXoQQc39",
        "7--X-vFxQx2V",
        "oqUeZlR6Rib8",
        "326amnuIT2Fc",
        "UcgNiniEWPIP"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}